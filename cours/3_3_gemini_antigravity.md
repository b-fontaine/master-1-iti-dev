# √âcosyst√®me Google AI Avanc√© : Gemini, NotebookLM et Antigravity

## Table des mati√®res

1. [Introduction](#1-introduction)
2. [Google Gemini : La famille de mod√®les](#2-google-gemini--la-famille-de-mod√®les)
   - 2.1 [Historique et √©volution](#21-historique-et-√©volution)
   - 2.2 [Gemini 3 Pro : Le mod√®le phare](#22-gemini-3-pro--le-mod√®le-phare)
   - 2.3 [Comparaison des mod√®les Gemini](#23-comparaison-des-mod√®les-gemini)
   - 2.4 [Capacit√©s multimodales](#24-capacit√©s-multimodales)
   - 2.5 [Function Calling et outils](#25-function-calling-et-outils)
   - 2.6 [Exemples de code avec l'API Gemini](#26-exemples-de-code-avec-lapi-gemini)
3. [Google NotebookLM](#3-google-notebooklm)
   - 3.1 [Concept et philosophie](#31-concept-et-philosophie)
   - 3.2 [Architecture RAG](#32-architecture-rag)
   - 3.3 [Fonctionnalit√©s principales](#33-fonctionnalit√©s-principales)
   - 3.4 [Cas d'usage pour la robotique](#34-cas-dusage-pour-la-robotique)
4. [Google Antigravity](#4-google-antigravity)
   - 4.1 [Pr√©sentation de la plateforme](#41-pr√©sentation-de-la-plateforme)
   - 4.2 [Architecture Agent-First](#42-architecture-agent-first)
   - 4.3 [Composants et workflow](#43-composants-et-workflow)
   - 4.4 [Int√©gration avec la robotique](#44-int√©gration-avec-la-robotique)
5. [Bonnes pratiques](#5-bonnes-pratiques)
6. [Ressources et r√©f√©rences](#6-ressources-et-r√©f√©rences)
7. [Glossaire](#7-glossaire)

---

## 1. Introduction

Ce document pr√©sente trois piliers majeurs de l'√©cosyst√®me d'intelligence artificielle de Google en 2025 :

- **Google Gemini** : La famille de mod√®les de langage multimodaux de Google
- **NotebookLM** : L'assistant de recherche ancr√© sur vos documents
- **Google Antigravity** : La plateforme de d√©veloppement agentique de nouvelle g√©n√©ration

Ces outils repr√©sentent l'√©volution de l'IA vers des syst√®mes plus intelligents, plus autonomes et plus int√©gr√©s. Pour les √©tudiants en robotique, comprendre ces technologies est essentiel car elles constituent les fondations des syst√®mes robotiques intelligents de demain.

```mermaid
graph TB
    subgraph "√âcosyst√®me Google AI 2025"
        G[Gemini<br/>Intelligence]
        N[NotebookLM<br/>M√©moire]
        A[Antigravity<br/>Action]
    end

    G --> |"Propulse"| N
    G --> |"Propulse"| A
    N --> |"Informe"| A

    style G fill:#4285F4,stroke:#fff,color:#fff
    style N fill:#34A853,stroke:#fff,color:#fff
    style A fill:#EA4335,stroke:#fff,color:#fff
```

> **Note pour les roboticiens** : Ces technologies permettent de cr√©er des robots capables de comprendre des instructions en langage naturel, de raisonner sur des t√¢ches complexes et d'interagir de mani√®re autonome avec leur environnement.

---

## 2. Google Gemini : La famille de mod√®les

### 2.1 Historique et √©volution

L'histoire de Gemini commence avec la fusion des √©quipes Google Brain et DeepMind en 2023, cr√©ant Google DeepMind. Cette consolidation a permis de d√©velopper une nouvelle g√©n√©ration de mod√®les multimodaux.

**Chronologie des versions Gemini :**

```mermaid
timeline
    title √âvolution de la famille Gemini
    section 2023
        D√©cembre 2023 : Gemini 1.0 (Pro, Ultra, Nano)
                      : Premier mod√®le multimodal natif
    section 2024
        F√©vrier 2024 : Gemini 1.5 Pro
                     : Fen√™tre de contexte 1M tokens
        Mai 2024 : Gemini 1.5 Flash
                 : Optimis√© pour la vitesse
        D√©cembre 2024 : Gemini 2.0 Flash
                      : Capacit√©s agentiques natives
    section 2025
        F√©vrier 2025 : Gemini 2.5 Pro/Flash
                     : Thinking Mode adaptatif
        Novembre 2025 : Gemini 3 Pro
                      : Raisonnement avanc√©
```

**Dates cl√©s :**

| Date | √âv√©nement | Signification |
|------|-----------|---------------|
| D√©cembre 2023 | Lancement Gemini 1.0 | Premier mod√®le multimodal natif de Google |
| F√©vrier 2024 | Gemini 1.5 Pro | Fen√™tre de contexte r√©volutionnaire (1M tokens) |
| D√©cembre 2024 | Gemini 2.0 Flash | Introduction des capacit√©s agentiques |
| Novembre 2025 | Gemini 3 Pro | Thinking Mode et raisonnement avanc√© |

### 2.2 Gemini 3 Pro : Le mod√®le phare

**Gemini 3 Pro** est le mod√®le de langage multimodal le plus avanc√© de Google (lanc√© en Preview en novembre 2025). Il repr√©sente un saut qualitatif majeur dans les capacit√©s de raisonnement.

#### Caract√©ristiques principales

| Caract√©ristique | Description | Impact |
|-----------------|-------------|--------|
| **Thinking Mode** | G√©n√®re une cha√Æne de pens√©e interne avant de r√©pondre | R√©duit les hallucinations de 60% |
| **Multimodalit√© native** | Traite texte, images, vid√©o, audio simultan√©ment | Compr√©hension holistique |
| **Contexte 1M+ tokens** | Ing√®re des livres entiers ou bases de code | Analyse de projets complets |
| **Tool Use natif** | Sait quand utiliser des outils externes | Autonomie accrue |

#### Le Thinking Mode expliqu√©

Le **Thinking Mode** est une innovation majeure de Gemini 3. Contrairement aux mod√®les pr√©c√©dents qui pr√©disaient directement le prochain token, Gemini 3 peut entrer dans un mode de "r√©flexion" :

```mermaid
flowchart LR
    subgraph "Mod√®le Classique"
        A1[Prompt] --> B1[Pr√©diction directe] --> C1[R√©ponse]
    end

    subgraph "Gemini 3 Thinking Mode"
        A2[Prompt] --> B2[Analyse]
        B2 --> C2[Cha√Æne de pens√©e]
        C2 --> D2[Validation]
        D2 --> E2[R√©ponse v√©rifi√©e]
    end
```

**Niveaux de Thinking :**

- **Low** : R√©flexion minimale, r√©ponses rapides
- **Medium** : √âquilibre entre vitesse et profondeur
- **High** : Raisonnement approfondi pour les t√¢ches complexes

> **Conseil** : Pour les applications robotiques critiques, utilisez le niveau "High" pour minimiser les erreurs de raisonnement.

#### Architecture conceptuelle

```mermaid
graph LR
    subgraph Inputs [Entr√©es Multimodales]
        A[Texte / Code]
        B[Images / Vid√©o]
        C[Audio]
        D[PDF / Docs]
    end

    subgraph Core [Gemini 3 PRO]
        E(Reasoning Engine<br/>Thinking Mode)
        F(Context Cache<br/>1M+ Tokens)
    end

    subgraph Tools [Outils Externes]
        G[Google Search]
        H[Code Interpreter]
        I[API Calls]
    end

    subgraph Outputs [Sorties]
        J[R√©ponse Structur√©e]
        K[Code Ex√©cutable]
        L[Contenu G√©n√©ratif]
    end

    Inputs --> Core
    Core <--> Tools
    Core --> Outputs
```

### 2.3 Comparaison des mod√®les Gemini

#### Tableau comparatif des mod√®les actuels (D√©cembre 2025)

| Mod√®le | Cas d'usage | Tokens entr√©e | Tokens sortie | Thinking | Function Calling |
|--------|-------------|---------------|---------------|----------|------------------|
| **Gemini 3 Pro** | Raisonnement avanc√©, agentic | 1,048,576 | 65,536 | ‚úÖ | ‚úÖ |
| **Gemini 2.5 Pro** | T√¢ches complexes, code | 1,048,576 | 65,536 | ‚úÖ | ‚úÖ |
| **Gemini 2.5 Flash** | Prix-performance optimal | 1,048,576 | 65,536 | ‚úÖ | ‚úÖ |
| **Gemini 2.5 Flash-Lite** | Ultra-rapide, co√ªt minimal | 1,048,576 | 65,536 | ‚úÖ | ‚úÖ |
| **Gemini 2.0 Flash** | Workloads g√©n√©raux | 1,048,576 | 8,192 | ‚ö†Ô∏è Exp | ‚úÖ |

#### Arbre de d√©cision pour le choix du mod√®le

```mermaid
flowchart TD
    A[Quel mod√®le choisir ?] --> B{Budget limit√© ?}
    B -->|Oui| C{Besoin de thinking ?}
    B -->|Non| D{T√¢che complexe ?}

    C -->|Oui| E[Gemini 2.5 Flash-Lite]
    C -->|Non| F[Gemini 2.0 Flash-Lite]

    D -->|Oui| G{Raisonnement critique ?}
    D -->|Non| H[Gemini 2.5 Flash]

    G -->|Oui| I[Gemini 3 Pro]
    G -->|Non| J[Gemini 2.5 Pro]

    style I fill:#4285F4,stroke:#fff,color:#fff
    style E fill:#34A853,stroke:#fff,color:#fff
```

### 2.4 Capacit√©s multimodales

Gemini est nativement multimodal, ce qui signifie qu'il a √©t√© entra√Æn√© d√®s le d√©part sur plusieurs types de donn√©es simultan√©ment.

#### Types d'entr√©es support√©es

| Type | Formats | Limite | Cas d'usage robotique |
|------|---------|--------|----------------------|
| **Texte** | Plain text, Markdown | 1M tokens | Instructions, documentation |
| **Images** | JPEG, PNG, WebP, GIF | Multiple images | Vision par ordinateur |
| **Vid√©o** | MP4, MOV, WebM | Heures de contenu | Analyse de sc√®nes |
| **Audio** | MP3, WAV, FLAC | Heures de contenu | Commandes vocales |
| **Documents** | PDF, DOCX | Centaines de pages | Manuels techniques |
| **Code** | Tous langages | Bases de code enti√®res | Analyse de firmware |

#### Diagramme du flux multimodal

```mermaid
flowchart TB
    subgraph "Entr√©es Robot"
        CAM[Cam√©ra RGB-D]
        MIC[Microphone]
        LIDAR[LiDAR]
        DOC[Documentation]
    end

    subgraph "Pr√©traitement"
        IMG[Images/Vid√©o]
        AUD[Audio]
        TXT[Texte/Donn√©es]
    end

    subgraph "Gemini 3"
        FUSION[Fusion Multimodale]
        REASON[Raisonnement]
    end

    subgraph "Sorties"
        CMD[Commandes Robot]
        EXPL[Explications]
        CODE[Code g√©n√©r√©]
    end

    CAM --> IMG
    MIC --> AUD
    LIDAR --> TXT
    DOC --> TXT

    IMG --> FUSION
    AUD --> FUSION
    TXT --> FUSION

    FUSION --> REASON
    REASON --> CMD
    REASON --> EXPL
    REASON --> CODE
```

### 2.5 Function Calling et outils

Le **Function Calling** permet √† Gemini d'interagir avec des syst√®mes externes de mani√®re structur√©e. C'est une capacit√© essentielle pour les applications robotiques.

#### Principe du Function Calling

```mermaid
sequenceDiagram
    participant U as Utilisateur
    participant G as Gemini
    participant F as Fonction Externe
    participant R as Robot

    U->>G: "D√©place le robot vers la cuisine"
    G->>G: Analyse et planification
    G->>F: move_robot(destination="kitchen")
    F->>R: Commande de mouvement
    R-->>F: Confirmation position
    F-->>G: {"status": "success", "position": "kitchen"}
    G-->>U: "Le robot est maintenant dans la cuisine"
```

#### Outils natifs disponibles

| Outil | Description | Cas d'usage |
|-------|-------------|-------------|
| **Google Search** | Recherche web en temps r√©el | Informations actualis√©es |
| **Code Execution** | Ex√©cution Python sandbox√©e | Calculs, visualisations |
| **Google Maps** | Donn√©es g√©ographiques | Navigation robotique |
| **URL Context** | Lecture de pages web | Documentation en ligne |
| **File Search** | Recherche dans fichiers upload√©s | Base de connaissances |

### 2.6 Exemples de code avec l'API Gemini

#### Installation et configuration

```bash
# Installation du SDK Python
pip install google-genai

# Ou avec npm pour JavaScript
npm install @google/genai
```

#### Exemple 1 : G√©n√©ration de texte simple

```python
from google import genai

# Configuration du client
client = genai.Client(api_key="VOTRE_CLE_API")

# G√©n√©ration simple
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Explique le concept de SLAM en robotique en 3 phrases."
)

print(response.text)
```

#### Exemple 2 : Analyse d'image pour la robotique

```python
from google import genai
from google.genai import types
import base64

client = genai.Client(api_key="VOTRE_CLE_API")

# Charger une image de la cam√©ra du robot
with open("camera_frame.jpg", "rb") as f:
    image_data = base64.b64encode(f.read()).decode()

# Analyse de la sc√®ne
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[
        types.Part.from_text("D√©cris les objets dans cette image et leur position relative. Identifie les obstacles potentiels pour un robot mobile."),
        types.Part.from_bytes(
            data=base64.b64decode(image_data),
            mime_type="image/jpeg"
        )
    ]
)

print(response.text)
```

#### Exemple 3 : Function Calling pour contr√¥le robotique

```python
from google import genai
from google.genai import types

# D√©finition des fonctions du robot
robot_functions = [
    {
        "name": "move_robot",
        "description": "D√©place le robot vers une position cible",
        "parameters": {
            "type": "object",
            "properties": {
                "x": {"type": "number", "description": "Coordonn√©e X en m√®tres"},
                "y": {"type": "number", "description": "Coordonn√©e Y en m√®tres"},
                "speed": {"type": "number", "description": "Vitesse en m/s (0.1-1.0)"}
            },
            "required": ["x", "y"]
        }
    },
    {
        "name": "grab_object",
        "description": "Saisit un objet avec le pr√©henseur",
        "parameters": {
            "type": "object",
            "properties": {
                "object_id": {"type": "string", "description": "Identifiant de l'objet"},
                "grip_force": {"type": "number", "description": "Force de pr√©hension en N"}
            },
            "required": ["object_id"]
        }
    }
]

client = genai.Client(api_key="VOTRE_CLE_API")
tools = types.Tool(function_declarations=robot_functions)
config = types.GenerateContentConfig(tools=[tools])

# Demande en langage naturel
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Va chercher la tasse rouge sur la table et apporte-la moi",
    config=config
)

# Traitement des appels de fonction
for part in response.candidates[0].content.parts:
    if hasattr(part, 'function_call'):
        fc = part.function_call
        print(f"Fonction: {fc.name}")
        print(f"Arguments: {fc.args}")
```

#### Exemple 4 : Thinking Mode pour planification complexe

```python
from google import genai
from google.genai import types

client = genai.Client(api_key="VOTRE_CLE_API")

# Configuration avec Thinking Mode
config = types.GenerateContentConfig(
    thinking_config=types.ThinkingConfig(
        thinking_budget=1024  # Tokens allou√©s √† la r√©flexion
    )
)

# T√¢che de planification complexe
response = client.models.generate_content(
    model="gemini-3-pro-preview",
    contents="""
    Tu es un planificateur de t√¢ches robotiques.

    Contexte: Un robot mobile avec un bras manipulateur doit ranger une pi√®ce.
    Objets d√©tect√©s: 3 livres sur le sol, 2 tasses sur le bureau, 1 chaise renvers√©e.
    Contraintes: Batterie √† 40%, capacit√© de charge 2kg max.

    G√©n√®re un plan d'action optimal en tenant compte des contraintes √©nerg√©tiques.
    """,
    config=config
)

print(response.text)
```

---

## 3. Google NotebookLM

### 3.1 Concept et philosophie

**NotebookLM** est un assistant de recherche "grounded" (ancr√©). Contrairement √† un chatbot g√©n√©rique qui utilise tout son entra√Ænement, NotebookLM se restreint **exclusivement** aux documents que vous lui fournissez.

#### Diff√©rence avec un chatbot classique

```mermaid
flowchart LR
    subgraph "Chatbot Classique"
        A1[Question] --> B1[Mod√®le + Tout Internet]
        B1 --> C1[R√©ponse potentiellement hallucin√©e]
    end

    subgraph "NotebookLM"
        A2[Question] --> B2[Recherche dans VOS documents]
        B2 --> C2[R√©ponse avec citations v√©rifiables]
    end
```

#### Avantages cl√©s

| Aspect | Chatbot classique | NotebookLM |
|--------|-------------------|------------|
| **Source** | Entra√Ænement g√©n√©ral | Vos documents uniquement |
| **V√©rifiabilit√©** | Difficile | Citations cliquables |
| **Hallucinations** | Fr√©quentes | Tr√®s rares |
| **Confidentialit√©** | Donn√©es partag√©es | Documents priv√©s |
| **Actualit√©** | Limit√© au cutoff | Toujours √† jour |

### 3.2 Architecture RAG

La technologie cl√© derri√®re NotebookLM est le **RAG** (Retrieval-Augmented Generation).

#### Principe du RAG

```mermaid
flowchart TB
    subgraph "Phase 1: Indexation"
        D[Documents] --> C[Chunking<br/>D√©coupage]
        C --> E[Embedding<br/>Vectorisation]
        E --> V[(Vector Store<br/>Base vectorielle)]
    end

    subgraph "Phase 2: Requ√™te"
        Q[Question utilisateur] --> QE[Query Embedding]
        QE --> S[Recherche s√©mantique]
        V --> S
        S --> R[Passages pertinents]
    end

    subgraph "Phase 3: G√©n√©ration"
        R --> P[Prompt enrichi]
        Q --> P
        P --> G[Gemini]
        G --> A[R√©ponse + Citations]
    end
```

#### √âtapes d√©taill√©es

1. **Chunking** : Les documents sont d√©coup√©s en segments de 500-1000 tokens
2. **Embedding** : Chaque segment est converti en vecteur (768-1536 dimensions)
3. **Indexation** : Les vecteurs sont stock√©s dans une base vectorielle
4. **Retrieval** : La question est vectoris√©e et compar√©e aux segments
5. **Augmentation** : Les segments pertinents enrichissent le prompt
6. **Generation** : Gemini g√©n√®re une r√©ponse bas√©e sur le contexte

### 3.3 Fonctionnalit√©s principales

#### Sources support√©es

| Type de source | Limite | Notes |
|----------------|--------|-------|
| Google Docs | 50 par notebook | Synchronisation automatique |
| PDF | 50 par notebook | OCR inclus |
| Sites web | 50 URLs | Extraction du contenu |
| YouTube | 50 vid√©os | Transcription automatique |
| Texte brut | 50 fichiers | Copier-coller |
| Google Slides | 50 pr√©sentations | Texte et notes |

#### Audio Overviews : La fonctionnalit√© star

NotebookLM peut transformer vos documents en un **podcast audio** r√©aliste avec deux h√¥tes IA qui discutent du contenu.

```mermaid
flowchart LR
    D[Documents] --> A[Analyse du contenu]
    A --> S[Script de podcast]
    S --> V1[Voix H√¥te 1]
    S --> V2[Voix H√¥te 2]
    V1 --> M[Mixage audio]
    V2 --> M
    M --> P[Podcast final]
```

**Caract√©ristiques :**
- Dur√©e : 5-15 minutes selon le contenu
- Ton conversationnel et engageant
- Analogies et exemples g√©n√©r√©s automatiquement
- Personnalisable (focus, ton, public cible)

### 3.4 Cas d'usage pour la robotique

#### Sc√©nario 1 : Documentation technique

```
Sources upload√©es:
- Manuel du robot (PDF)
- Datasheets des capteurs (PDF)
- Notes de cours ROS 2 (Google Docs)

Questions possibles:
- "Comment calibrer le LiDAR selon le manuel ?"
- "Quelles sont les limites de couple des moteurs ?"
- "R√©sume les diff√©rences entre ROS 1 et ROS 2"
```

#### Sc√©nario 2 : Revue de litt√©rature

```
Sources upload√©es:
- 20 articles de recherche sur le SLAM (PDF)
- Th√®ses de doctorat (PDF)

Questions possibles:
- "Compare les approches SLAM visuelles vs LiDAR"
- "Quels sont les d√©fis non r√©solus mentionn√©s ?"
- "G√©n√®re un Audio Overview de l'√©tat de l'art"
```

#### Diagramme de s√©quence : Flux NotebookLM

```mermaid
sequenceDiagram
    participant U as Utilisateur
    participant N as NotebookLM
    participant E as Embedding Store
    participant G as Gemini

    U->>N: Upload de Documents (PDF, Docs)
    N->>E: Cr√©ation des Vector Embeddings
    Note over N,E: Les documents deviennent la "Source de V√©rit√©"

    U->>N: Pose une question
    N->>E: Recherche s√©mantique (Retrieval)
    E-->>N: Retourne les passages pertinents
    N->>G: Envoie Question + Contexte
    G-->>N: G√©n√®re r√©ponse avec citations
    N-->>U: Affiche r√©ponse + Notes de bas de page
```

---

## 4. Google Antigravity

### 4.1 Pr√©sentation de la plateforme

**Google Antigravity** est la plateforme de d√©veloppement "Agent-First" de nouvelle g√©n√©ration, lanc√©e en novembre 2025 par Google DeepMind. Elle repr√©sente l'√©volution de l'IDE vers l'√®re des agents autonomes.

> "Antigravity n'est pas juste un IDE avec de l'IA. C'est un environnement o√π les agents autonomes prennent en charge des t√¢ches complexes de bout en bout."
> ‚Äî Blog Google Developers, Novembre 2025

#### Positionnement dans l'√©cosyst√®me

```mermaid
flowchart TB
    subgraph "√âvolution des outils de d√©veloppement"
        A[IDE Classique<br/>2000s] --> B[IDE + Autocompl√©tion<br/>2010s]
        B --> C[IDE + Copilot<br/>2020-2023]
        C --> D[IDE Agent-First<br/>2024+]
    end

    subgraph "Exemples"
        A1[Eclipse, VS Code]
        B1[IntelliSense]
        C1[GitHub Copilot, Cursor]
        D1[Antigravity, Windsurf]
    end

    A --- A1
    B --- B1
    C --- C1
    D --- D1

    style D fill:#EA4335,stroke:#fff,color:#fff
    style D1 fill:#EA4335,stroke:#fff,color:#fff
```

### 4.2 Architecture Agent-First

#### Diff√©rence avec les assistants de code traditionnels

| Aspect | Copilot/Cursor | Antigravity |
|--------|----------------|-------------|
| **Paradigme** | Autocompl√©tion assist√©e | Agent autonome |
| **Scope** | Ligne/fonction | Projet entier |
| **Planification** | Aucune | Multi-√©tapes |
| **V√©rification** | Manuelle | Automatique |
| **Contexte** | Fichier courant | Codebase + Web + Docs |

#### Les trois piliers d'Antigravity

```mermaid
graph TB
    subgraph "1. Planification"
        P1[Analyse du prompt]
        P2[D√©composition en t√¢ches]
        P3[Cr√©ation du plan]
    end

    subgraph "2. Ex√©cution"
        E1[√âdition de code]
        E2[Cr√©ation de fichiers]
        E3[Ex√©cution de commandes]
    end

    subgraph "3. V√©rification"
        V1[Tests automatiques]
        V2[Inspection visuelle]
        V3[Validation fonctionnelle]
    end

    P1 --> P2 --> P3
    P3 --> E1
    E1 --> E2 --> E3
    E3 --> V1 --> V2 --> V3
    V3 -->|Erreur| P1
    V3 -->|Succ√®s| F[Livraison]
```

### 4.3 Composants et workflow

#### Composants cl√©s

| Composant | R√¥le | Technologie |
|-----------|------|-------------|
| **Agent Manager** | Orchestration des t√¢ches | Gemini 3 Pro |
| **IDE instrument√©** | Lecture/√©criture de code | Fork VS Code |
| **Browser Agent** | Tests visuels automatis√©s | Puppeteer/Playwright |
| **Terminal Agent** | Ex√©cution de commandes | Shell sandbox√© |
| **Artefacts** | Suivi et documentation | Markdown dynamique |

#### Workflow agentique complet

```mermaid
graph TD
    User([Utilisateur]) -->|"Prompt: 'Cr√©e une API REST'"| Manager[Antigravity Manager]

    subgraph "Cycle Autonome"
        Manager --> Planner[Agent Planificateur]
        Planner -->|Plan & Task List| Artifacts[(Artefacts)]

        Manager --> Coder[Agent Codeur]
        Coder -->|√âdition de Fichiers| IDE[Projet / File System]

        Manager --> Verifier[Agent V√©rificateur]
        IDE -->|Launch App| Browser[Browser Agent]
        Verifier -->|Inspecte| Browser

        Manager --> Terminal[Agent Terminal]
        Terminal -->|npm install, tests| IDE
    end

    Verifier -->|Feedback / Erreur| Manager
    Manager -->|Succ√®s| User

    style Manager fill:#4285F4,stroke:#fff,color:#fff
    style Artifacts fill:#FBBC04,stroke:#fff
    style Browser fill:#EA4335,stroke:#fff,color:#fff
```

#### Exemple de session Antigravity

```
üë§ Utilisateur: "Cr√©e une API REST pour contr√¥ler un bras robotique avec
   des endpoints pour move, grip et status"

ü§ñ Antigravity:
   üìã Plan cr√©√©:
   1. Analyser les requirements
   2. Cr√©er la structure du projet
   3. Impl√©menter les endpoints
   4. Ajouter la validation
   5. √âcrire les tests
   6. Documenter l'API

   ‚öôÔ∏è Ex√©cution en cours...
   ‚úÖ Fichier cr√©√©: src/app.py
   ‚úÖ Fichier cr√©√©: src/robot_controller.py
   ‚úÖ Fichier cr√©√©: tests/test_api.py
   ‚úÖ Tests pass√©s: 12/12

   üì¶ Livrable: API REST fonctionnelle avec documentation Swagger
```

### 4.4 Int√©gration avec la robotique

#### Architecture Robot + Antigravity

```mermaid
flowchart TB
    subgraph "D√©veloppement (Antigravity)"
        AG[Agent Antigravity]
        CODE[Code Robot]
        SIM[Simulateur]
    end

    subgraph "D√©ploiement"
        ROS[ROS 2 Node]
        HW[Hardware Robot]
    end

    subgraph "Feedback Loop"
        LOG[Logs & T√©l√©m√©trie]
        DEBUG[D√©bogage assist√©]
    end

    AG --> CODE
    CODE --> SIM
    SIM -->|Validation| ROS
    ROS --> HW
    HW --> LOG
    LOG --> DEBUG
    DEBUG --> AG
```

#### Cas d'usage : D√©veloppement d'un n≈ìud ROS 2

```python
# Prompt Antigravity:
# "Cr√©e un n≈ìud ROS 2 qui publie les donn√©es d'un capteur IMU
#  √† 100Hz avec gestion des erreurs"

# Code g√©n√©r√© par Antigravity:
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Imu
from rclpy.qos import QoSProfile, ReliabilityPolicy
import numpy as np

class IMUPublisher(Node):
    def __init__(self):
        super().__init__('imu_publisher')

        # Configuration QoS pour donn√©es temps r√©el
        qos = QoSProfile(
            depth=10,
            reliability=ReliabilityPolicy.BEST_EFFORT
        )

        self.publisher = self.create_publisher(Imu, 'imu/data', qos)
        self.timer = self.create_timer(0.01, self.publish_imu)  # 100Hz

        self.get_logger().info('IMU Publisher initialis√©')

    def publish_imu(self):
        try:
            msg = Imu()
            msg.header.stamp = self.get_clock().now().to_msg()
            msg.header.frame_id = 'imu_link'

            # Lecture capteur (simul√©e ici)
            msg.linear_acceleration.x = np.random.normal(0, 0.1)
            msg.linear_acceleration.y = np.random.normal(0, 0.1)
            msg.linear_acceleration.z = np.random.normal(9.81, 0.1)

            self.publisher.publish(msg)

        except Exception as e:
            self.get_logger().error(f'Erreur lecture IMU: {e}')

def main():
    rclpy.init()
    node = IMUPublisher()
    rclpy.spin(node)
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

---

## 5. Bonnes pratiques

### 5.1 Utilisation de Gemini

#### Prompting efficace

| Pratique | Exemple | Pourquoi |
|----------|---------|----------|
| **√ätre sp√©cifique** | "G√©n√®re du code Python 3.10 pour ROS 2 Humble" | √âvite les ambigu√Øt√©s |
| **Donner du contexte** | "Tu es un expert en robotique mobile..." | Oriente les r√©ponses |
| **Structurer la demande** | "1. Analyse 2. Impl√©mente 3. Teste" | Facilite le suivi |
| **Demander des explications** | "Explique chaque √©tape" | Am√©liore la compr√©hension |

#### Gestion des tokens

```mermaid
flowchart LR
    A[Prompt long] --> B{> 100k tokens ?}
    B -->|Oui| C[Utiliser Context Caching]
    B -->|Non| D[Envoi direct]
    C --> E[√âconomie de co√ªts]
    D --> F[R√©ponse rapide]
```

### 5.2 Utilisation de NotebookLM

#### Organisation des sources

- **Grouper par th√®me** : Un notebook par projet/sujet
- **Nommer clairement** : Pr√©fixer les documents (01_intro, 02_methode...)
- **Mettre √† jour** : Supprimer les sources obsol√®tes
- **Limiter le scope** : 10-20 sources pertinentes > 50 sources vagues

#### Questions efficaces

```
‚ùå "Parle-moi du SLAM"
‚úÖ "Selon les articles upload√©s, quelles sont les 3 principales
   limitations du SLAM visuel en environnement ext√©rieur ?"
```

### 5.3 Utilisation d'Antigravity

#### Prompts de qualit√©

```
‚ùå "Fais une app"

‚úÖ "Cr√©e une application Flask qui:
   - Expose une API REST pour contr√¥ler un robot
   - Endpoints: /move (POST), /status (GET), /stop (POST)
   - Validation des entr√©es avec Pydantic
   - Tests unitaires avec pytest
   - Documentation Swagger automatique"
```

#### Revue du travail de l'agent

- **V√©rifier le plan** avant l'ex√©cution
- **Lire les artefacts** g√©n√©r√©s
- **Tester manuellement** les fonctionnalit√©s critiques
- **It√©rer** avec des feedbacks pr√©cis

---

## 6. Ressources et r√©f√©rences

### 6.1 Documentation officielle

| Ressource | URL | Description |
|-----------|-----|-------------|
| **Gemini API Docs** | ai.google.dev/gemini-api | Documentation compl√®te |
| **Google AI Studio** | aistudio.google.com | Interface web pour tester |
| **NotebookLM** | notebooklm.google.com | Acc√®s √† l'outil |
| **Antigravity** | antigravity.dev | Plateforme de d√©veloppement |

### 6.2 Tutoriels et guides

**Gemini :**
- [Quickstart Python](https://ai.google.dev/gemini-api/docs/quickstart)
- [Function Calling Guide](https://ai.google.dev/gemini-api/docs/function-calling)
- [Multimodal Prompting](https://ai.google.dev/gemini-api/docs/vision)

**Robotique + IA :**
- [Gemini Robotics Overview](https://ai.google.dev/gemini-api/docs/robotics)
- [ROS 2 + LLM Integration](https://docs.ros.org/)

### 6.3 Communaut√©s

- **Discord** : Google AI Developers
- **Reddit** : r/GoogleGeminiAI, r/robotics
- **GitHub** : google-gemini, google-deepmind
- **Stack Overflow** : Tags [google-gemini], [notebooklm]

### 6.4 Articles de recherche

| Titre | Auteurs | Ann√©e | Sujet |
|-------|---------|-------|-------|
| Gemini: A Family of Highly Capable Multimodal Models | Google DeepMind | 2023 | Architecture Gemini |
| PaLM-E: An Embodied Multimodal Language Model | Google | 2023 | LLM pour robotique |
| RT-2: Vision-Language-Action Models | Google DeepMind | 2023 | Contr√¥le robotique |

---

## 7. Glossaire

| Terme | D√©finition |
|-------|------------|
| **Agent IA** | Syst√®me autonome capable de percevoir, raisonner et agir |
| **Antigravity** | Plateforme de d√©veloppement agent-first de Google |
| **Context Window** | Nombre maximum de tokens qu'un mod√®le peut traiter |
| **Embedding** | Repr√©sentation vectorielle d'un texte ou m√©dia |
| **Function Calling** | Capacit√© d'un LLM √† invoquer des fonctions externes |
| **Gemini** | Famille de mod√®les multimodaux de Google |
| **Grounding** | Ancrage des r√©ponses sur des sources v√©rifiables |
| **Hallucination** | G√©n√©ration d'informations fausses par un LLM |
| **LLM** | Large Language Model - Mod√®le de langage de grande taille |
| **Multimodal** | Capable de traiter plusieurs types de donn√©es (texte, image, audio) |
| **NotebookLM** | Assistant de recherche ancr√© sur documents de Google |
| **RAG** | Retrieval-Augmented Generation - G√©n√©ration augment√©e par r√©cup√©ration |
| **Thinking Mode** | Mode de raisonnement explicite de Gemini 3 |
| **Token** | Unit√© de texte (mot ou sous-mot) trait√©e par un LLM |
| **Tool Use** | Capacit√© d'un mod√®le √† utiliser des outils externes |
| **Vector Store** | Base de donn√©es optimis√©e pour les vecteurs d'embedding |

---

## Synth√®se

```mermaid
mindmap
  root((√âcosyst√®me<br/>Google AI))
    Gemini
      Mod√®les
        3 Pro
        2.5 Pro/Flash
        2.0 Flash
      Capacit√©s
        Multimodal
        Thinking Mode
        Function Calling
      Usage
        API
        AI Studio
    NotebookLM
      Concept
        RAG
        Grounding
      Features
        Audio Overviews
        Citations
      Usage
        Recherche
        Synth√®se
    Antigravity
      Architecture
        Agent-First
        IDE instrument√©
      Composants
        Planner
        Coder
        Verifier
      Usage
        D√©veloppement
        Automatisation
```

**En r√©sum√© :**

- **Gemini** est le "Cerveau" ‚Äî L'intelligence brute multimodale
- **NotebookLM** est le "Biblioth√©caire" ‚Äî La m√©moire et la synth√®se ancr√©e
- **Antigravity** est l'"Ouvrier" ‚Äî L'action et le d√©veloppement autonome

Ces trois outils, utilis√©s ensemble, permettent de cr√©er des syst√®mes robotiques intelligents capables de comprendre, raisonner et agir de mani√®re autonome.
